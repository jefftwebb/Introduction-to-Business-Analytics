# Modeling



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, warnings = F)

```


## tl;dr

This chapter introduces the classification tree algorithm, focusing on key concepts like entropy and information gain, along with the R code for fitting a tree using the `rpart` package and its eponymous `rpart()` function. While classification tree models typically don't perform as well as some other machine learning algorithms, they nevertheless produce a clear set of decision rules which can be very useful for segmenting a customer population. The `rpart` package is an open-source implementation of the CART algorithm (Classification and Regression Trees), developed by Leo Breiman and others in the 1980s.  ("rpart" stands for "recursive partitioning.")

## Introduction
In the last chapter  we cleaned the MegaTelCo data, removing rows with impossible or improbable values, as well as those with missing values.  We then explored the data looking for notable relationships between the target variable and various predictors.  We are now ready to model churn more formally.  The method we will use, as an illustration of modeling generally in the analytics lifecycle, is decision or classification trees, as implemented in the R package, `rpart`. (This is just one of many packages implementing decision trees in R.)  Before we get started, let's remind ourselves of the broader context of what we are doing.

This is a binary **classification problem** because churn has two possible outcomes---represented  in the target factor variable with the class labels, `LEAVE` and `STAY`.    And it is a **supervised problem** because we have historical observations of both leaving and staying, which can be used to  create a model of the relationship between various predictor variables--- demographic, behavioral, and what we have called attitudinal---and the target variable.  The problem can also be described as **supervised segmentation** because we are using that historical data to  create a model that will segment future customers into one of two groups: leave or stay.   (Remember:   if the outcome is continuous  then the problem is a regression problem, not a classification problem.) 

A secondary goal of the analytics project described in the Business Problem Statement was to identify customers most  likely to respond to a special re-enrollment offer.  However, the modeling demonstrated in this tutorial is solely focused on the first goal:  Create a model to predict which customers will churn.

## Data

First, load packages, including `rpart` and `rpart.plot` for fitting and visualizing tree models.  We will reproduce the cleaning from the previous chapter. Additionally, because our objective in this chapter is modeling, we will remove the ID variable, since, as an arbitrarily assigned number, it should have no predictive value. 

```{r}
#Load libraries
library(tidyverse)
library(rpart)
library(rpart.plot)

# Load the data
m <- read.csv("megatelco.csv")

# Clean the data
m %>% 
  mutate(reported_satisfaction = factor(reported_satisfaction,
                                        levels = c("low","avg", "high"),
                                        ordered = T),
         reported_usage_level = factor(reported_usage_level,
                                       levels = c("low",  "avg", "high"),
                                       ordered = T),
         considering_change_of_plan = factor(considering_change_of_plan,
                                       levels = c("no", "maybe", "yes")),
         leave = factor(leave),
         college = ifelse(college == "one", 1, 0)) %>% 
  filter(income > 0,
         house > 0,
         handset_price < 1000) %>% 
  na.omit %>% 
  select(-id) -> m_clean 

# Inspect data
str(m_clean)
```

As a reference, here is the data dictionary:

```{r echo = F}
data.frame(Variable = c("College", "Income",  "Overage", "Leftover", "House", "Handset_price", "Over_15mins_calls_per_month", "Average_call_duration", "Reported_satisfaction", "Reported_usage_level", "Considering_change_of_plan", "Leave","ID"),
            Explanation = c("Is the customer college educated?"," Annual income","Average overcharges per month","Average % leftover minutes per month","Estimated value of dwelling (from the census tract)", "Cost of phone", "Average number of long calls (15 mins or over) per month", "Average duration of a call", "Reported level of satisfaction", "Self-reported usage level", "Was customer considering changing his/her plan?","Class variable: whether customer left or stayed", "Numeric customer ID")) %>%
  kable
```


## Majority Class

It is important at the outset of classification modeling to determine the proportions of labels---the factor levels---in the target variable, in order to identify the majority class. In this case the target variable, representing customer churn, is `leave`.  What are the proportions of `LEAVE` and `STAY`? In the last chapter we saw how to to do that calculation:  

```{r proptable}
mean(m_clean$leave == "LEAVE") 
mean(m_clean$leave == "STAY") 
```

Suppose that we would like to predict whether a new customer will churn. One option, without any further modeling, would be to use the majority class in the target variable of the historical data as the prediction. This would not be a very good model.  Based on the historical data predictions from this majority class model would be correct about 51% of the time. But that would be *slightly* better than random guessing. Any model we create should therefore have better performance than the majority class model.

## Tree Model

We will be using the `rpart` package in R, an implementation of the [CART algorithm](https://en.wikipedia.org/wiki/Decision_tree_learning), to fit a classification tree model to `leave` in the MegaTelCo data. (`rpart` stands for "Recursive Partitioning and Regression Trees"; see the [package vignette](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf) for implementation details.)  The model syntax used in this package is typical for R, consisting in a function, `rpart()`, with formula and data arguments.  Formulas in R typically take the form of `y ~ x + z`, with the `y` standing for the variable to be modeled, the tilde indicating "modeled by" or "explained by," and the predictors, here `x` and `z`, included additively.  

Let's demonstrate how to use  `rpart` for classification by fitting a simple tree---call it a stump---of `leave` with just one predictor, `house`, which we noticed in  the previous tutorial seemed somewhat correlated with the target variable. As a reminder, here is the plot: 

```{r boxplot}

ggplot(data = m_clean, 
       mapping = aes(x = leave, y = house)) +
  geom_boxplot() +
  labs(title = "leave ~ house")
  
```

Here is the tree:

```{r tree, echo=T}
# Fit tree
tree_model <- rpart(formula = leave ~ house, 
                     data = m_clean)

tree_model
```

I have assigned the model to an object, `tree_model`, which we will use later.  For now, let's work through these simplified results. One thing to keep in mind is that due to the factor structure in the target variable---due to the alphabetic defaults for the `factor()` function, the first level is `LEAVE` and the second level is `STAY`---the algorithm is specifically modeling `STAY`

- `n =`: n is the number of rows in the data set: 4994.
- `1) root`:  The root node by definition has no split, and consists in the entire dataset: 4994 observations.  The next number, 2468, referred to as "loss" in this output, is the count of misclassifications that would occur if the majority class label in the node, `STAY`, were used for prediction. The proportions of `LEAVE` and `STAY` in the data are .49 and .51, respectively.
- `2) house < 600255.5`: the first split is on `house` at 600,225.5, with the left branch (`house< 600,255.5`) producing a group of size 3306, with loss of 1364 incorrectly classified observations.  The class label prediction for this node is the majority label, `LEAVE`, accounting for almost 59% of the observations in the node. This is a leaf node, as indicated by the asterisk.
- `3) house >= 600255.5`: the right branch produces a group of 1688, with loss of 526.  The class label prediction for this node is the majority label, `STAY`, accounting for almost 69% of the observations in this leaf node.

A classification tree can be visualized as an upside down tree with the "root" at the top and "nodes" at each split or decision point, with the "leaves" at the terminal nodes. Here is a visualization of the tree using the `rpart.plot` function:

```{r tree-plot}
# Plot tree object
rpart.plot(x = tree_model)

```

`rpart.plot()` simply plots a model object created by `rpart()`, which we define in the `x` argument to the function. 

How to read this plot?  66% of the data (3306 observations) is in the left branch and 34% (1688 observations) in the right. The majority class in the left leaf node is `LEAVE`, with 1 - .41 = .59 or 59% of the observations, which is therefore the prediction for all observations falling in this node.  The reason the tree lists the proportion of `STAY` in the left branch is because, as noted above, that is the category we are explicitly modeling due to the variable's factor structure. The majority class, and hence the predicted class for that group, in the right node is `STAY` with 69% of the observations.

So, a tree model is super simple to set up in R, though the output can be tricky to interpret.  It helps to have an example handy when interpreting a new tree model.

Notice that, while `rpart()` has created a model of `STAY`, the model provides information about both `STAY` and `LEAVE`, telling us when---according to which values of `house`---to predict one and when to predict the other. 

I'm using the word "predict" here in a possibly misleading way, since this model is not predicting, in the sense of *forecasting* whether or not a customer will churn in the future, but rather *describing* when a customer *has* churned.  The model is simply describing patterns in the existing data.  However, the aim of creating such a model is to use it eventually for prediction, so we will loosely refer to the model as "predicting" the outcome in this case even when we already know the outcome.

Let's use this simple tree model to review the concepts of purity, entropy and information gain.

## Purity and Entropy

Purity is a commonsense notion.  The leaf nodes in a tree model are pure if they all have the same class label.  If they don't---as in our simple model---then they are impure.  For example, the observed outcomes in the leaf node in the left branch consist in 41% `STAY` and 59% `LEAVE`, and, in the right branch, 69% `STAY` and 31% `LEAVE`.  These leaf nodes are impure, then, but how impure? Entropy answers that question for us, formalizing the notion of impurity.   Entropy ranges between 0 and 1, with 0 indicating  no  disorder (the group is pure)  and 1 indicating maximal disorder (the group is balanced).

Figure 3-3 in *Data Science for Business* is helpful:

![](images/entropy.png)

In our our tree model, entropy would be 0 if the leaf nodes were either all `LEAVE` or all `STAY`.  The nodes are mixed though, and as the mixture of these two classes increases, so does entropy.  If the two classes were balanced 50-50 within a node (at the top of the curve in 3-3) then the node would have maximum disorder and entropy would be 1. 

Entropy is  formally defined as: $-p_1 log(p_1) - p_2 log(p_2) ...$

We will use log base 2 to calculate entropy and information gain, rather than natural log (which is more common in statistics), because it ensures that entropy scales between 0 and 1. (Natural log would work just fine for calculating entropy for comparative use but it would not have this nice property.)

Entropy in the two leaf nodes, using rounded inputs from the plot for illustration, would be:

- Left leaf node:  -.41 x log(.41) - .59 x log(.59) = .98.
- Right leaf node:  -.69 x log(.69) - .31 x log(.31) = .89.

This result makes intuitive sense since entropy is higher in the more impure left node where the classes are more evenly balanced. 

For future calculations we will not round the inputs since our final result would then be thrown off by rounding error.

## Information Gain

Entropy thus inversely measures the purity of a node: as purity goes down entropy goes up.  It is a node-level metric.  Information gain (IG), by contrast, measures the *change in node purity* resulting from a split. The goal in a creating a classification tree is, of course, to find splits that make nodes with the least possible entropy or disorder (ideally, nodes would be pure, with all of the observations having the same class label). After a split the two nodes (known as children) should together have higher purity (lower entropy) than the original node before the split (known as the parent). We use IG comparatively to evaluate possible splits with the goal of identifying the one split out of many candidates that most decreases entropy (increases purity) in the children relative to the parent. The concept is fairly simple, though the formula looks ferocious and is tricky to use: 


$$IG(parent, children) = entropy(parent) - [p(c_1) entropy(c_1) + p(c_2) entropy(c_2) + ...]$$

The formula combines the weighted entropy in the children (weighted by the proportion of the data in each node) and subtracts it from the entropy in the parent.

The $IG$ calculation for the tree model, using all the decimals we have available from the output, would be:

- $entropy(parent)$ uses the proportion of `LEAVE` and `STAY` in the parent node, which in this case is the entire dataset: -.4941930 x log(.4941930) - .5058070 x log(.5058070) = .9999027.
- $p(c_1)$  is the proportion of observations  in the left branch node: 3306/4994 = .6619944
- $p(c_2)$ is the proportion of observations in the right branch node: 1 - .66 = .3380056.
- $entropy(c_1)$ from above: -.4125832 x log(.4125832) - .5874168 x log(.5874168) = .977837.
- $entropy(c_2)$ from above: -.6883886 x log(.6883886) - .3116114 x log(.3116114) = .8950248.

$IG$ = .9999027 - (.66 x .977837 + .3380056 x .8950248) = .05.

It is fine to round the final result for reporting purposes.


Below is a plot showing the exact $IG$ associated with each potential split on house. The value of `house` at maximum $IG$ is about 600,000 (black dashed line) as identified by the tree model.

```{r echo = F}
# Create a data.frame of house prices from min to max by 1000
# and an empty column for storing the corresponding entropy
# df <- data.frame(house = seq(min(m_clean$house) + 1000, max(m_clean$house), by = 500),
#                  entropy = 0,
#                  info_gain = 0)
# 
# # Check the data frame
# # head(df)
# 
# # parent entropy
# parent_entropy <- m_clean %>%
#   dplyr::summarize(p1 = mean(ifelse(leave=="STAY", 1, 0)),
#             p2 = 1-p1,
#             entropy = -p1*log2(p1)-p2*log2(p2)) %>%
#   dplyr::select(entropy) %>% 
#   as.numeric
# 
# # Set up a loop to compute entropies
# for(i in seq_along(df$house)){
#   temp <- m_clean %>%
#     mutate(predict = ifelse(house > df$house[i], "LEAVE", "STAY"),
#            leave = leave) %>%
#     count(predict, leave) %>%
#     group_by(predict) %>%
#     mutate(perc = n/sum(n),
#            entropy = -(first(perc)* log2(first(perc)) + last(perc)* log2(last(perc)))) %>% 
#     group_by(predict) %>% 
#     mutate(total = sum(n)) %>%
#     slice(1) %>% 
#     dplyr::select(predict, entropy, total) %>% 
#     ungroup %>% 
#     mutate(perc=total/sum(total)) 
#   
#   df$entropy[i] <- -(temp$perc[1] * log2(temp$perc[1]) + temp$perc[2] * log2(temp$perc[2]))
#     
#   df$info_gain[i] <- parent_entropy - (temp$perc[1] * temp$entropy[1] +
#                                                temp$perc[2] * temp$entropy[2])
# }
#   
# 
# 
# names(df)[3] <- "Information gain"
# 
# write_csv(df, "ig_table.csv")


df <- read_csv("ig_table.csv")

ggplot(df, aes(house, `Information gain`)) +
  geom_line() +
  geom_vline(xintercept = as.integer(df[which(df$`Information gain`==max(df$`Information gain`)), "house"]), lty=2, col = 2)+
  labs(title="Information gain at different splits on house",
       subtitle="Max IG in red",
       y = "information gain") + 
  theme_minimal()

```


## Greediness

The classification tree algorithm does an efficient search for the best split among all the available predictors---the split that most increases IG.  The search is "greedy" in the sense that the algorithm looks for the best split conditional on all the previous splits. This makes the splits locally optimal, with no guarantee of being globally optimal.

>A greedy algorithm is an algorithmic paradigm that follows the problem solving heuristic of making the locally optimal choice at each stage with the intent of finding a global optimum. In many problems, a greedy strategy does not usually produce an optimal solution, but nonetheless a greedy heuristic may yield locally optimal solutions that approximate a globally optimal solution in a reasonable amount of time. [Wikipedia](https://en.wikipedia.org/wiki/Greedy_algorithm)

In other words, a greedy algorithm always chooses the best step at a given point, which is not necessarily (though probably gets close to) the best step overall.

The tree is finished when the algorithm can find no additional information gain-improving splits.  The resulting "leaf" nodes ideally define subgroups or segments within the data that have low entropy.  The majority class in each leaf node will be used as the model's prediction for observations sharing those characteristics in new data when the outcome is unknown.


## Model Accuracy

Is this a good model?  How would we know?  One of the most common measures of classification model performance is accuracy, defined as the proportion of correct classifications (hence ranging from 0 to 1). We can calculate accuracy using the the model output:

```{r tree_mod}
tree_model
```

The left branch node has 3306 observations, 1364 of which were incorrect.  The right branch node has 1688 observations, 526 of which were incorrect.  Accuracy will thus be:  ((3306 - 1364) + (1688 - 526)) / 4994 = .62.  62% of the model's classifications were correct, and 1 - .62 = .38 is the proportion of *incorrect* classifications.

This number can be obtained more simply using the `predict()` function.  The `predict()` function extracts from the model object the class label prediction for each row:


```{r predict_tree}
predict(object = tree_model, 
        type = "class") %>% 
  head
```

Using this output we can calculate accuracy as follows:


```{r acc}
(predict(object = tree_model, type = "class") == m_clean$leave) %>% 
  mean
```

This code works because the double equals sign, "==," evaluates to `TRUE` or `FALSE` (in effect it asks: does the *predicted* value equal the *observed* value, T or F?), and in R the `mean()` function counts `TRUE` as 1.  The code sums the instances where predicted is identical to observed and divides by the number of rows.



## Model Improvement

Can we improve the model?   Let's fit a tree model with all the  available predictors . We will use shorthand notation--- "~ ."---for including all the predictors additively: 

```{r}
# Fit tree
(tree_model2 <- rpart(formula = leave ~ ., 
                      data = m_clean))


# Visualize
rpart.plot(x = tree_model2)
```

Let's practice reading this tree. The first split is on house less than \$600k (left branch) or greater than \$600k (right branch). Subsequently, all the splits will have "yes" on the left and "no" on the right, where "yes" and "no" represent answers to the condition stated in the split. Reading the left branch: customers owning homes worth less than \$600k (first split) and with overage greater than 98 (second split) are predicted to leave. Reading the right branch: customers owning homes worth more than \$600k (first split) and with incomes less than \$100k (second split) are predicted to stay.

Plots with lots of nodes and branches can quickly become too complicated to view. But we can adjust the `tweak` argument and the `roundint` argument to the plotting function to make a more legible plot. Play around with different numbers. `tweak = 1` will produce the same plot as above.

```{r}
rpart.plot(tree_model2, tweak = 1, roundint=T)
```

Notice in the model summary that there are missing nodes---the count jumps from 13 to 22.  The missing nodes have been "pruned" by the algorithm after growing the tree because they did not improve model performance.


How would we interpret this model? Notice that the very top of the tree gives a key: observations are classified into the right branch when the condition stated in the node is false ("no") and classified into the left branch when true ("yes"). Take the rightmost leaf in the tree as an example.  If house > 600k and income is less than 100k then the model predicts `STAY`. 


Has the addition of predictors improved the model's accuracy?


```{r}
# Evaluate accuracy
(predict(tree_model2, type = "class") == m_clean$leave)  %>% 
  mean

```

The model's accuracy has improved to .71. According to the accuracy metric this is a better model than the one with a single predictor, `house`.

What does this mean?  Well, in terms of a marketing campaign, we should be realistic about the fact that something like 30% of the customers predicted to churn, and to whom we might extend a special retention offer, will have been incorrectly classified.  (The number might actually be higher than that; we will ignore the possibility for now that the model might be overfitting.) We need to factor this error--- and uncertainty about how large the error is likely to be in practice---into any recommended approach to improving customer retention at MegaTelCo.

## Variable Importance

We can get a sense of which variables are most important in predicting churn by looking at the tree plot:  the splitting variables closer to the root node are more important. Clearly `house` is the strongest predictor, followed by `overage` and `income`.  This can be useful when doing explanatory or descriptive modeling in order to understand which factors contribute to churn.  However, it is important to understand that variable importance does not imply causation.  For example, we would not want to say that certain housing values cause churn!  Notice that the surprising finding from EDA was confirmed by the tree model:  customer satisfaction is not a predictor of churn.

## Summary

**Packages**

- dplyr
- ggplot2
- rpart
- rpart.plot

**Functions**

- `read.csv()`. Reads a .csv file into R's working memory for analysis. The equivalent function in the tidyverse is `read_csv()`, which will do pretty much the same thing.
- `%>%`. A pipe. Shortcut is shift-command-m. Used for separating data operations into discrete steps, for readability.
- `ggplot()`.  Created a blank plot canvas.  Must be used in conjunction with a `geom` function specifying the plot type and the aesthetic mapping. 
- `aes()`.  The aesthetic argument to the `ggplot` function. Specifies which variables 
Map to the ex and Y axis, and which is associated with an additional plot dimension such as color.
- `geom_boxplot()`. Creates a boxplot.
- `rpart()`.  The workhorse function for fitting a classification or a regression tree using the `rpart` package. Note that if the target variable is coded numerically the function will automatically create a regression tree.
- `rpart.plot()`.  Visualizes a classification or regression tree object created by `rpart()`. This function is included in the `rpart.plot` package. Tree visualizations can be quite busy, making them hard to read.  Two arguments to `rpart.plot()`, `tweak` and `roundint`, are useful for adjusting the size of the text and the nodes.

**Glossary**

- `Predictive model`. A formula for estimating the unknown value of interest: the target. The formula could be mathematical, or it could be a logical statement, such as a rule. Often it is a hybrid of the two. A predictive model of churn would tell us which customers are likely to churn.
- `Predict`. Estimate an unknown value.
- `Descriptive model`. A model that provides insight into the underlying phenomenon or process. A descriptive model of churn behavior would tell us what customers who churn typically look like.
- `Supervised learning`. A model that describes a relationship between a set of selected variables (attributes or features) and a predefined variable called the target variable. The model estimates the value of the target variable as a function (possibly a probabilistic function) of the features.
- `Attributes`. Fields, columns, variables, features, explanatory variables, predictors.
- `Instance or example`. Represents a fact or a data point, Also known as a row in a database or spreadsheet.
- `Feature vector`. An instance, represented as a fixed-length, ordered collection (vector) of feature values.
- `Modeling`. The creation of a model from data, sometimes called model induction. Induction is a term from philosophy that refers to generalizing from specific cases to general rules (in contrast to deduction, which starts with general rules and specific facts, and creates other specific facts from them).
- `Training data`. The input data used for creating the model.
- `Supervised segmentation`. Using historical data to divide customers into segments. In the case of the churn problem, supervised segmentation involves dividing customers into segments having (on average) a higher or lower tendency to leave the company after contract expiration.
- `Informative attributes`. Variables that reduce our uncertainty about the target variable. These are the variables that are closer to the root in visualization of the tree, and that are most strongly correlated with the target variable. 
- `Purity`. A "pure" group or class is homogeneous with respect to the target variable. That is, if every member of the group has the same target label, then the group is pure. If there is at least one member of the group that has a different target label, then the group is impure.
- `Entropy`. A measure of disorder that can be applied to a set to indicate how mixed or impure it is with respect to a property of interest.
- `Information gain`. A measure of how much an attribute improves (decreases) entropy over the whole segmentation creates. It measures the change in entropy due to any amount of new information being added.
- `Greediness`.The tree algorithm is "greedy" in the sense that it looks for the best split of the data  conditional on all the previous splits. This makes the splits locally optimal, with no guarantee of being globally optimal.
- `Classification tree`. A segmentation of the data in which every data point corresponds to one and only one path in the tree and thereby to one and only one week. Each leaf corresponds to a segment, and the attributes and values along the path give the characteristics of the segment.
- `Accuracy`.  A measure of performance specific to classification models. Accuracy is defined as the proportion of correct predictions. 

